{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "INFINNERATOR_example.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amrosnik/adventure_scrape/blob/master/INFINNERATOR_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdPRwOL8PPw3",
        "colab_type": "text"
      },
      "source": [
        "InFINNerator: an RNN for generating Finn the Human speech"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0TYdZZFPOu4",
        "colab_type": "text"
      },
      "source": [
        "Here's my first attempt at creating an RNN. I will be using a homemade dataset of Adventure Time transcripts of Finn dialogue to generate \"new\" Finn speech! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sawo9xv-Py5z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVTsGLExQJtB",
        "colab_type": "text"
      },
      "source": [
        "Let's first look at the data. I've outlined elsewhere how I formatted the dataset, but for now let's unpack the pickled Finn data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bL6XWC3ZY69h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import unicodedata\n",
        "import string\n",
        "import pickle\n",
        "import random\n",
        "import numpy as np\n",
        "import cloudpickle as cp\n",
        "from urllib.request import urlopen\n",
        "import time\n",
        "import math\n",
        "import torch.optim as optim\n",
        "\n",
        "## initial inspiration from from https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html \n",
        "torch.manual_seed(1)\n",
        "all_letters = string.punctuation + string.digits + string.ascii_lowercase + string.whitespace\n",
        "n_letters = len(all_letters)+1\n",
        "url = 'https://github.com/amrosnik/adventure_scrape/raw/master/finn_just_dialogue.pkl'\n",
        "finn_data = cp.load(urlopen(url)) \n",
        "\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "        and c in all_letters\n",
        "    )\n",
        "finn_text = [unicodeToAscii(line) for line in finn_data]\n",
        "finn_dialogue = [line for line in finn_text if len(line) > 0 ]\n",
        "random.shuffle(finn_dialogue)\n",
        "\n",
        "# concatenate all lines into one big corpus:\n",
        "finn_corpus = '\\n'.join(str(elem) for elem in finn_dialogue)\n",
        "finn_corpus = list(finn_corpus)\n",
        "finn_corpus = np.asarray(finn_corpus)\n",
        "\n",
        "## separate dataset into lines of equal number of chars \n",
        "line_size = 64\n",
        "num_lines = round(len(finn_corpus)/line_size)\n",
        "randomized_finn = [(finn_corpus[i:i+line_size]) for i in range(0, len(finn_corpus), line_size)]\n",
        "\n",
        "# Find letter index from all_letters, e.g. \"a\" = 0\n",
        "def letterToIndex(letter):\n",
        "    return all_letters.find(letter)\n",
        "\n",
        "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
        "def letterToTensor(letter):\n",
        "    tensor = torch.zeros(1, n_letters)\n",
        "    tensor[0][letterToIndex(letter)] = 1\n",
        "    return tensor\n",
        "\n",
        "# Turn a line into a <line_length x 1 x n_letters>,\n",
        "# or an array of one-hot letter vectors\n",
        "# One-hot matrix of first to last letters (not including EOS) for input\n",
        "def lineToTensor(line):\n",
        "    tensor = torch.zeros(len(line), 1, n_letters)\n",
        "    for li, letter in enumerate(line):\n",
        "        tensor[li][0][letterToIndex(letter)] = 1\n",
        "    return tensor\n",
        "\n",
        "# LongTensor of second letter to end (EOS) for target\n",
        "def targetTensor(line):\n",
        "    letter_indexes = [all_letters.find(line[li]) for li in range(1, len(line))]\n",
        "    return torch.LongTensor(letter_indexes)\n",
        "\n",
        "   # Random item from a list\n",
        "def randomChoice(l):\n",
        "      return l[random.randint(0, len(l) - 1)]\n",
        "\n",
        "# Make category, input, and target tensors from a random category, line pair\n",
        "def randomTrainingExample(minibatch_k=32):\n",
        "    limits = len(finn_corpus)-(line_size+1)\n",
        "    idxs = np.asarray(np.random.randint(0,limits,size=minibatch_k))\n",
        "    inputs = np.array([finn_corpus[idx:idx+line_size] for idx in idxs])\n",
        "    targets = np.array([finn_corpus[(idx+1):idx+1+line_size] for idx in idxs])\n",
        "    inputs = inputs.T\n",
        "    targets = targets.T\n",
        "    input_line_tensor = torch.zeros(line_size, minibatch_k, n_letters,dtype=torch.float32)  ## size 64,32,101\n",
        "    for line in range(len(inputs[0])):\n",
        "       for li, letter in enumerate(inputs[line]):\n",
        "          #print(i,line,letter)\n",
        "          input_line_tensor[li][line][letterToIndex(letter)] = 1\n",
        "    target_line_tensor = torch.zeros(line_size, minibatch_k,dtype=torch.long)\n",
        "    for line in range(len(targets[0])):\n",
        "       for li,letter in enumerate(targets[line]):\n",
        "          target_line_tensor[li][line] = letterToIndex(letter)\n",
        "    return inputs,targets,input_line_tensor, target_line_tensor "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0A1p4Csn7fTH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.autograd import Variable\n",
        "\n",
        "class CharRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
        "        super(CharRNN, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        self.rnn = nn.LSTM(input_size, hidden_size, n_layers,batch_first=False)\n",
        "        self.decoder = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        batch_size = input.size(0)\n",
        "        input_view = input.view(1,batch_size,-1)\n",
        "        output, hidden = self.rnn(input_view, hidden)\n",
        "        output = self.decoder(output[0])\n",
        "        output = self.softmax(output)\n",
        "        return output, hidden\n",
        "\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return (Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size)),\n",
        "               Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUWvooCpm8MZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.NLLLoss()\n",
        "\n",
        "learning_rate = 0.0001\n",
        "hidden_size = 512\n",
        "batch_size = 32\n",
        "rnn = CharRNN(n_letters, hidden_size,n_letters,n_layers=2)\n",
        "optimizer = optim.Adam(rnn.parameters(), lr=learning_rate)\n",
        "\n",
        "def train(ins,ts,input_line_tensor, target_line_tensor):\n",
        "    hc = rnn.init_hidden(batch_size)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    loss = torch.empty(1)\n",
        "    output = torch.empty((input_line_tensor.size(1),input_line_tensor.size(2)),requires_grad=True)\n",
        "\n",
        "    seq_len = input_line_tensor.size(0)\n",
        "    for i in range(seq_len):\n",
        "        output, hc = rnn(input_line_tensor[i], hc)\n",
        "        loss += criterion(output, target_line_tensor[i]) / seq_len\n",
        "    if (loss > 10):\n",
        "      print(loss,ins)\n",
        "    loss.backward()\n",
        "  \n",
        "    optimizer.step()\n",
        "    return output, loss.item()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9w9rtl_1HMC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def timeSince(since):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "  \n",
        "n_iters = 5000\n",
        "print_every = 50\n",
        "plot_every = 50\n",
        "all_losses = []\n",
        "total_loss = 0 # Reset every plot_every iters\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for iter in range(1, n_iters + 1):\n",
        "    output, loss = train(*randomTrainingExample())\n",
        "    total_loss += loss\n",
        "    \n",
        "    if iter % print_every == 0:\n",
        "        print('%s (%d %d%%) %.4f' % (timeSince(start), iter, iter / n_iters * 100, total_loss / print_every))\n",
        "        total_loss = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtJA4v_A1SSN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_loss = 0\n",
        "n_iters = 250\n",
        "for iter in range(1, n_iters + 1):\n",
        "    output, loss = train(*randomTrainingExample())\n",
        "    total_loss += loss\n",
        "    \n",
        "    if iter % print_every == 0:\n",
        "        print('%s (%d %d%%) %.4f' % (timeSince(start), iter, iter / n_iters * 100, total_loss / print_every))\n",
        "        total_loss = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-d16SD7xzH2",
        "colab_type": "code",
        "outputId": "9a6d9a4b-1364-44bb-8394-13f5105a3d73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "max_length = 100\n",
        "# Sample from a category and starting letter\n",
        "temperature = 1.0\n",
        "\n",
        "def sample(start_letter='A', temperature=1.5):\n",
        "    with torch.no_grad():  # no need to track history in sampling\n",
        "        input = lineToTensor(start_letter)\n",
        "        hidden = rnn.init_hidden(1)\n",
        "\n",
        "        output_name = start_letter\n",
        "        \n",
        "        for i in range(max_length):\n",
        "            output, hidden = rnn(input[0], hidden)\n",
        "            \n",
        "            exp_output = torch.exp(temperature * output)\n",
        "            top_i = torch.multinomial(exp_output, 1)[0]\n",
        "            letter = all_letters[top_i]\n",
        "            output_name += letter\n",
        "            input = lineToTensor(letter)\n",
        "\n",
        "        return output_name\n",
        "\n",
        "# Get multiple samples from one category and multiple starting letters\n",
        "def samples(start_letters='abc'):\n",
        "    for start_letter in start_letters:\n",
        "        print(sample(start_letter))\n",
        "  \n",
        "batch_size = 32\n",
        "hidden_size = 128\n",
        "prime_sample = random.choice(all_letters)\n",
        "test = sample(prime_sample)\n",
        "print(test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "}ltgmh  am r t  .eoehol ,oo lo  !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}