{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "INFINNERATOR.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amrosnik/adventure_scrape/blob/master/INFINNERATOR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdPRwOL8PPw3",
        "colab_type": "text"
      },
      "source": [
        "InFINNerator: an RNN for generating Finn the Human speech"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0TYdZZFPOu4",
        "colab_type": "text"
      },
      "source": [
        "Here's my first attempt at creating an RNN. I will be using a homemade dataset of Adventure Time transcripts of Finn dialogue to generate \"new\" Finn speech! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sawo9xv-Py5z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVTsGLExQJtB",
        "colab_type": "text"
      },
      "source": [
        "Let's first look at the data. I've outlined elsewhere how I formatted the dataset, but for now let's unpack the pickled Finn data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bL6XWC3ZY69h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "3b22ea7c-7f4a-49ff-c7e2-f3ed73b592f8"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import unicodedata\n",
        "import string\n",
        "import pickle\n",
        "import random\n",
        "import cloudpickle as cp\n",
        "from urllib.request import urlopen\n",
        "import time\n",
        "import math\n",
        "import torch.optim as optim\n",
        "\n",
        "all_letters = string.printable #string.ascii_letters + \" .,;'!?-\"\n",
        "n_letters = len(all_letters)+1\n",
        "url = 'https://github.com/amrosnik/adventure_scrape/raw/master/finn_just_dialogue.pkl'\n",
        "finn_data = cp.load(urlopen(url)) \n",
        "\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "        and c in all_letters\n",
        "    )\n",
        "finn_text = [unicodeToAscii(line) for line in finn_data]\n",
        "finn_dialogue = [line for line in finn_text if len(line) > 0 ]\n",
        "random.shuffle(finn_dialogue)\n",
        "\n",
        "# concatenate all lines into one big corpus:\n",
        "finn_corpus = ''.join(str(elem) for elem in finn_dialogue)\n",
        "#print(len(finn_corpus))\n",
        "batch_size = round(len(finn_corpus) / 10)\n",
        "batch_remainder = abs(len(finn_corpus)-batch_size*10) ## add remainder to first batch\n",
        "#print(batch_size*10)\n",
        "\n",
        "start_batch = [i*batch_size +i for i in range(10)]\n",
        "end_batch = [(i+1)*batch_size + i for i in range(10)]\n",
        "#print(start_batch,end_batch)\n",
        "\n",
        "torch.manual_seed(1)\n",
        "## ripped from https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html \n",
        "\n",
        "# Find letter index from all_letters, e.g. \"a\" = 0\n",
        "def letterToIndex(letter):\n",
        "    return all_letters.find(letter)\n",
        "\n",
        "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
        "def letterToTensor(letter):\n",
        "    tensor = torch.zeros(1, n_letters)\n",
        "    tensor[0][letterToIndex(letter)] = 1\n",
        "    return tensor\n",
        "\n",
        "# Turn a line into a <line_length x 1 x n_letters>,\n",
        "# or an array of one-hot letter vectors\n",
        "# One-hot matrix of first to last letters (not including EOS) for input\n",
        "def lineToTensor(line):\n",
        "    tensor = torch.zeros(len(line), 1, n_letters)\n",
        "    for li, letter in enumerate(line):\n",
        "        tensor[li][0][letterToIndex(letter)] = 1\n",
        "    #print(tensor)\n",
        "    return tensor\n",
        "\n",
        "# LongTensor of second letter to end (EOS) for target\n",
        "def targetTensor(line):\n",
        "    letter_indexes = [all_letters.find(line[li]) for li in range(1, len(line))]\n",
        "    #print('before ',letter_indexes)\n",
        "    #for i in range(len(letter_indexes)):\n",
        "    #   print(line[i],letter_indexes[i])\n",
        "    letter_indexes.append(n_letters - 1) # EOS\n",
        "    #print('after append ',letter_indexes)\n",
        "    return torch.LongTensor(letter_indexes)\n",
        "#print(letterToTensor('F'))\n",
        "#print(lineToTensor('Finn').size())\n",
        "#print(lineToTensor(finn_dialogue[0]).size())\n",
        "\n",
        "   # Random item from a list\n",
        "def randomChoice(l):\n",
        "      return l[random.randint(0, len(l) - 1)]\n",
        "\n",
        "# Make category, input, and target tensors from a random category, line pair\n",
        "def randomTrainingExample():\n",
        "    line = randomChoice(finn_dialogue)\n",
        "    input_line_tensor = lineToTensor(line)\n",
        "    target_line_tensor = targetTensor(line)\n",
        "    #print(input_line_tensor, target_line_tensor)\n",
        "    return input_line_tensor, target_line_tensor\n",
        "  \n",
        "for i in range(10):\n",
        "    input_line_tensor, target_line_tensor = randomTrainingExample()\n",
        "    #print(input_line_tensor.size(), target_line_tensor.size())\n",
        "  "
      ],
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 29831, 59662, 89493, 119324, 149155, 178986, 208817, 238648, 268479] [29830, 59661, 89492, 119323, 149154, 178985, 208816, 238647, 268478, 298309]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAMI9LS5gy4G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### create batches then one-hot? \n",
        "### or one-hot everything then make into batches? \n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(RNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
        "        #self.o2o = nn.Linear(hidden_size + output_size, output_size)\n",
        "        #self.dropout = nn.Dropout(0.1)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        input_combined = torch.cat((input, hidden), 1)\n",
        "        hidden = self.i2h(input_combined)\n",
        "        output = self.i2o(input_combined)\n",
        "        #output_combined = torch.cat((hidden, output), 1)\n",
        "        #output = self.o2o(output_combined)\n",
        "        #output = self.dropout(output)\n",
        "        output = self.softmax(output)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, self.hidden_size)\n",
        "            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUWvooCpm8MZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1717
        },
        "outputId": "d987aa88-1b95-41a2-e97a-e0d8eb90946a"
      },
      "source": [
        "criterion = nn.NLLLoss()\n",
        "\n",
        "learning_rate = 0.001\n",
        "rnn = RNN(n_letters, 128, n_letters)\n",
        "optimizer = optim.Adam(rnn.parameters(), lr=learning_rate)\n",
        "\n",
        "def train(input_line_tensor, target_line_tensor):\n",
        "    #print('before ',target_line_tensor.size())\n",
        "    target_line_tensor.unsqueeze_(-1) #OG code: target_line_tensor.unsqueeze_(-1)\n",
        "    #print(target_line_tensor.size())\n",
        "    hidden = rnn.initHidden()\n",
        "\n",
        "    #rnn.zero_grad()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    #loss = 0\n",
        "    loss = torch.empty(1)\n",
        "    #print('before loop',loss)\n",
        "    output = torch.empty((input_line_tensor.size(1),input_line_tensor.size(2)),requires_grad=True)\n",
        "    #print(input_line_tensor.size(0))\n",
        "\n",
        "    #print(target_line_tensor)\n",
        "    for i in range(input_line_tensor.size(0)):\n",
        "        #print(i,input_line_tensor.size(),input_line_tensor.size(0))\n",
        "        output, hidden = rnn(input_line_tensor[i], hidden)\n",
        "        #print(i,input_line_tensor[i],target_line_tensor[i])\n",
        "        #print(target_line_tensor[i],target_line_tensor)\n",
        "        l = criterion(output, target_line_tensor[i])\n",
        "        loss += l\n",
        "        #print(l,loss)\n",
        "\n",
        "    #print('after loop',loss)\n",
        "    loss.requires_grad_()\n",
        "    #print('after loop II',loss)\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "    #for p in rnn.parameters():\n",
        "    #    p.data.add_(-learning_rate, p.grad.data)\n",
        "\n",
        "    #print(input_line_tensor.size(0), loss.item() / input_line_tensor.size(0))\n",
        "    return output, loss.item() / input_line_tensor.size(0)\n",
        "  \n",
        "\n",
        "#def train_batch\n",
        "#    n_epochs = 100 # or whatever\n",
        "#    batch_size = 128 # or whatever\n",
        "\n",
        "#    for epoch in range(n_epochs):\n",
        "\n",
        "#        # X is a torch Variable\n",
        " #       permutation = torch.randperm(X.size()[0])\n",
        "\n",
        "#        for i in range(0,X.size()[0], batch_size):\n",
        "#            optimizer.zero_grad()\n",
        "\n",
        " #           indices = permutation[i:i+batch_size]\n",
        " #           batch_x, batch_y = X[indices], Y[indices]\n",
        "\n",
        "            # in case you wanted a semi-full example\n",
        " #           outputs = model.forward(batch_x)\n",
        " #           loss = lossfunction(outputs,batch_y)\n",
        "\n",
        "#            loss.backward()\n",
        "#            optimizer.step()\n",
        "\n",
        "  \n",
        "def timeSince(since):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "  \n",
        "n_iters = 50000\n",
        "print_every = 500\n",
        "plot_every = 50\n",
        "all_losses = []\n",
        "total_loss = 0 # Reset every plot_every iters\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for iter in range(1, n_iters + 1):\n",
        "    output, loss = train(*randomTrainingExample())\n",
        "    total_loss += loss\n",
        "    #print(loss)\n",
        "    \n",
        "    #print('out',loss)\n",
        "    if iter % print_every == 0:\n",
        "        print('%s (%d %d%%) %.4f' % (timeSince(start), iter, iter / n_iters * 100, loss))\n",
        "\n",
        "    if iter % plot_every == 0:\n",
        "        all_losses.append(total_loss / plot_every)\n",
        "        total_loss = 0\n"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0m 5s (500 1%) 3.4234\n",
            "0m 11s (1000 2%) 2.5835\n",
            "0m 18s (1500 3%) 2.5751\n",
            "0m 24s (2000 4%) 2.4448\n",
            "0m 30s (2500 5%) 2.3699\n",
            "0m 37s (3000 6%) 2.3530\n",
            "0m 42s (3500 7%) 2.2161\n",
            "0m 48s (4000 8%) 2.4279\n",
            "0m 55s (4500 9%) 2.6430\n",
            "1m 2s (5000 10%) 2.1516\n",
            "1m 9s (5500 11%) 1.9186\n",
            "1m 16s (6000 12%) 2.0725\n",
            "1m 22s (6500 13%) 2.0845\n",
            "1m 28s (7000 14%) 2.1284\n",
            "1m 34s (7500 15%) 1.9891\n",
            "1m 40s (8000 16%) 2.1381\n",
            "1m 46s (8500 17%) 2.4220\n",
            "1m 52s (9000 18%) 2.1813\n",
            "1m 58s (9500 19%) 1.4975\n",
            "2m 5s (10000 20%) 2.2425\n",
            "2m 11s (10500 21%) 2.1420\n",
            "2m 17s (11000 22%) 2.1963\n",
            "2m 23s (11500 23%) 2.1403\n",
            "2m 29s (12000 24%) 2.9414\n",
            "2m 36s (12500 25%) 2.3185\n",
            "2m 42s (13000 26%) 1.6302\n",
            "2m 49s (13500 27%) 2.0853\n",
            "2m 56s (14000 28%) 1.7070\n",
            "3m 3s (14500 28%) 1.7714\n",
            "3m 10s (15000 30%) 2.2004\n",
            "3m 17s (15500 31%) 1.9982\n",
            "3m 23s (16000 32%) 2.3702\n",
            "3m 29s (16500 33%) 1.9283\n",
            "3m 36s (17000 34%) 2.0042\n",
            "3m 42s (17500 35%) 2.4992\n",
            "3m 48s (18000 36%) 2.2224\n",
            "3m 55s (18500 37%) 2.0622\n",
            "4m 1s (19000 38%) 4.0587\n",
            "4m 7s (19500 39%) 2.0385\n",
            "4m 13s (20000 40%) 2.0816\n",
            "4m 19s (20500 41%) 1.8202\n",
            "4m 25s (21000 42%) 1.9802\n",
            "4m 32s (21500 43%) 2.3061\n",
            "4m 38s (22000 44%) 2.3429\n",
            "4m 44s (22500 45%) 1.2236\n",
            "4m 50s (23000 46%) 1.2356\n",
            "4m 58s (23500 47%) 2.0768\n",
            "5m 5s (24000 48%) 2.1977\n",
            "5m 12s (24500 49%) 2.6387\n",
            "5m 19s (25000 50%) 1.7930\n",
            "5m 25s (25500 51%) 2.3883\n",
            "5m 32s (26000 52%) 1.9002\n",
            "5m 38s (26500 53%) 2.3464\n",
            "5m 44s (27000 54%) 2.1578\n",
            "5m 50s (27500 55%) 2.1515\n",
            "5m 56s (28000 56%) 2.0630\n",
            "6m 2s (28500 56%) 2.4165\n",
            "6m 8s (29000 57%) 2.5820\n",
            "6m 15s (29500 59%) 2.2288\n",
            "6m 21s (30000 60%) 2.1611\n",
            "6m 27s (30500 61%) 1.9070\n",
            "6m 32s (31000 62%) 2.4324\n",
            "6m 38s (31500 63%) 2.0925\n",
            "6m 45s (32000 64%) 2.4099\n",
            "6m 52s (32500 65%) 1.7738\n",
            "6m 59s (33000 66%) 2.6089\n",
            "7m 7s (33500 67%) 1.9539\n",
            "7m 13s (34000 68%) 1.9904\n",
            "7m 19s (34500 69%) 2.3711\n",
            "7m 25s (35000 70%) 2.2227\n",
            "7m 31s (35500 71%) 2.0481\n",
            "7m 38s (36000 72%) 3.3582\n",
            "7m 44s (36500 73%) 3.0023\n",
            "7m 50s (37000 74%) 2.3148\n",
            "7m 57s (37500 75%) 2.4563\n",
            "8m 3s (38000 76%) 2.0608\n",
            "8m 9s (38500 77%) 2.0719\n",
            "8m 15s (39000 78%) 2.2947\n",
            "8m 21s (39500 79%) 1.9903\n",
            "8m 27s (40000 80%) 1.7139\n",
            "8m 33s (40500 81%) 2.2772\n",
            "8m 40s (41000 82%) 1.6022\n",
            "8m 47s (41500 83%) 2.5845\n",
            "8m 54s (42000 84%) 2.1151\n",
            "9m 1s (42500 85%) 3.1313\n",
            "9m 7s (43000 86%) 1.8434\n",
            "9m 13s (43500 87%) 2.0412\n",
            "9m 19s (44000 88%) 1.9141\n",
            "9m 26s (44500 89%) 1.9283\n",
            "9m 32s (45000 90%) 1.8596\n",
            "9m 38s (45500 91%) 2.1499\n",
            "9m 45s (46000 92%) 2.2288\n",
            "9m 51s (46500 93%) 2.1204\n",
            "9m 57s (47000 94%) 1.9196\n",
            "10m 4s (47500 95%) 1.8054\n",
            "10m 10s (48000 96%) 1.9697\n",
            "10m 16s (48500 97%) 2.4292\n",
            "10m 22s (49000 98%) 1.9039\n",
            "10m 28s (49500 99%) 2.2762\n",
            "10m 34s (50000 100%) 2.0270\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-d16SD7xzH2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "b8131d1d-671c-480e-cfd1-abb76d186cd1"
      },
      "source": [
        "max_length = 100\n",
        "\n",
        "# Sample from a category and starting letter\n",
        "def sample(start_letter='A'):\n",
        "    with torch.no_grad():  # no need to track history in sampling\n",
        "        input = lineToTensor(start_letter)\n",
        "        hidden = rnn.initHidden()\n",
        "\n",
        "        output_name = start_letter\n",
        "\n",
        "        for i in range(max_length):\n",
        "            output, hidden = rnn(input[0], hidden)\n",
        "            topv, topi = output.topk(1)\n",
        "            topi = topi[0][0]\n",
        "            if topi == n_letters - 1:\n",
        "                break\n",
        "            else:\n",
        "                letter = all_letters[topi]\n",
        "                output_name += letter\n",
        "            input = lineToTensor(letter)\n",
        "\n",
        "        return output_name\n",
        "\n",
        "# Get multiple samples from one category and multiple starting letters\n",
        "def samples(start_letters='ABC'):\n",
        "    for start_letter in start_letters:\n",
        "        print(sample(start_letter))\n",
        "\n",
        "samples('sds')"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "s me bed you dont ing the s all the s all the s all the s all the s all the s all the s all the s all\n",
            "d you dont ing the s all the s all the s all the s all the s all the s all the s all the s all the s \n",
            "s me bed you dont ing the s all the s all the s all the s all the s all the s all the s all the s all\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}